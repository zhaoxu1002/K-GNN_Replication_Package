# =============================================================================
# ğŸ“Š Tail Analysis: Diagnosing the "Where do we win?" (Final Fix)
# ä¿®å¤ç‚¹:
# 1. Tensor .cpu().numpy() è½¬æ¢ (è§£å†³ value_counts æŠ¥é”™)
# 2. loss_type='BPR' (è§£å†³è´Ÿé‡‡æ ·é…ç½®å†²çª)
# 3. Numpy 2.0 å…¼å®¹è¡¥ä¸
# =============================================================================

import sys
import os
import numpy as np
import pandas as pd

# ğŸ”¥ 0. Numpy 2.0 è¡¥ä¸ (å¿…é¡»åœ¨æœ€å‰é¢)
patch_map = {
    'float_': np.float64, 'int_': np.int64, 'bool_': bool,
    'complex_': np.complex128, 'object_': object,
    'unicode_': np.str_, 'string_': np.bytes_, 'str_': np.str_,
    'float': float, 'int': int
}
for alias, target in patch_map.items():
    if not hasattr(np, alias): setattr(np, alias, target)

import torch
from recbole.config import Config
from recbole.data import create_dataset, data_preparation
from recbole.utils import init_seed
from recbole.model.sequential_recommender import SASRec

def analyze_performance_by_popularity():
    print("ğŸ•µï¸â€â™‚ï¸ æ­£åœ¨æ‰§è¡Œã€é•¿å°¾æ€§èƒ½åˆ†æã€‘...")
    
    # 1. é‡æ–°åŠ è½½é…ç½®
    current_path = os.getcwd()
    dataset_path = os.path.join(current_path, 'dataset')
    if not os.path.exists(dataset_path):
        dataset_path = os.path.join(current_path, 'K-GNN_Replication_Package', 'dataset')
    
    print(f"   ğŸ“‚ æ•°æ®é›†è·¯å¾„: {dataset_path}")

    config_dict = {
        'data_path': dataset_path,
        'dataset': 'MOOCCubeX',
        'load_col': {'inter': ['user_id', 'item_id', 'timestamp']},
        # âœ… ä¿®å¤ 1: æ˜¾å¼æŒ‡å®š loss_type='BPR'
        'loss_type': 'BPR',
        'train_neg_sample_args': {'distribution': 'uniform', 'sample_num': 1},
        'eval_args': {'split': {'LS': 'valid_and_test'}, 'group_by': 'user', 'order': 'TO', 'mode': 'uni100'},
        'metrics': ['NDCG', 'Hit'], 'topk': [10],
        'seed': 2024, 'gpu_id': 0, 'use_gpu': True,
        'MAX_ITEM_LIST_LENGTH': 5,
        'state': 'INFO'
    }
    
    try:
        config = Config(model=SASRec, dataset='MOOCCubeX', config_dict=config_dict)
        init_seed(config['seed'], config['reproducibility'])
        dataset = create_dataset(config)
        train_data, valid_data, test_data = data_preparation(config, dataset)
    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
        return
    
    # 2. è®¡ç®—ç‰©å“æµè¡Œåº¦ (Popularity)
    print("   ğŸ“Š è®¡ç®—ç‰©å“æµè¡Œåº¦åˆ†å¸ƒ...")
    
    # âœ… ä¿®å¤ 2: Tensor -> Numpy è½¬æ¢
    # RecBole çš„ inter_feat['item_id'] æ˜¯ä¸ª Tensorï¼Œä¸èƒ½ç›´æ¥ value_counts
    item_id_tensor = dataset.inter_feat['item_id']
    if torch.is_tensor(item_id_tensor):
        item_id_numpy = item_id_tensor.cpu().numpy()
    else:
        item_id_numpy = item_id_tensor
        
    item_freq = pd.Series(item_id_numpy).value_counts().sort_index()
    
    # è¡¥å…¨é‚£äº›æ²¡å‡ºç°çš„ item ä¸º 0
    all_items = np.arange(dataset.item_num)
    freq_map = pd.Series(0, index=all_items)
    freq_map.update(item_freq)
    
    # é™¤å» padding (0)
    if 0 in freq_map.index: freq_map = freq_map.drop(0)
    
    print(f"   ç»Ÿè®¡æ¦‚è§ˆ:\n{freq_map.describe()}")
    
    # å®šä¹‰åˆ†ç»„ (Hot/Cold)
    def get_group(count):
        if count <= 5: return "1_Very_Cold (<5)"
        elif count <= 10: return "2_Cold (5-10)"
        elif count <= 20: return "3_Mid (10-20)"
        elif count <= 50: return "4_Warm (20-50)"
        else: return "5_Hot (>50)"
        
    group_map = freq_map.apply(get_group)
    
    # 3. åˆ†ææµ‹è¯•é›†åˆ†å¸ƒ
    print("\n   ğŸ¯ åˆ†ææµ‹è¯•é›† (Test Set) çš„ç›®æ ‡ç‰©å“åˆ†å¸ƒ...")
    target_items = []
    
    # éå† DataLoader
    for batch_idx, batched_data in enumerate(test_data):
        # å…¼å®¹å¤„ç†ï¼šæœ‰äº›ç‰ˆæœ¬è¿”å› tupleï¼Œæœ‰äº›ç›´æ¥è¿”å› Interaction å¯¹è±¡
        interaction = batched_data[0] if isinstance(batched_data, (tuple, list)) else batched_data
        
        if 'item_id' in interaction:
            target_items.extend(interaction['item_id'].cpu().numpy())
            
    if not target_items:
        print("âš ï¸ æ— æ³•ä» Loader æå–ï¼Œè·³è¿‡è¯¦ç»†åˆ†å¸ƒåˆ†æã€‚")
        return

    target_groups = [group_map.get(i, "Unknown") for i in target_items]
    target_group_counts = pd.Series(target_groups).value_counts().sort_index()
    
    print("\n   ğŸ§ª æµ‹è¯•é›†ç‰©å“çƒ­åº¦åˆ†å¸ƒ (Model Exam Questions):")
    total = len(target_items)
    for g in target_group_counts.index:
        count = target_group_counts[g]
        pct = (count / total) * 100
        print(f"   {g}: {count} ({pct:.2f}%)")
    
    # 4. è¯Šæ–­ç»“è®º
    hot_ratio = (target_group_counts.get("5_Hot (>50)", 0) + target_group_counts.get("4_Warm (20-50)", 0)) / total
    print("\n   ğŸ’¡ æ ¸å¿ƒè¯Šæ–­:")
    print(f"   çƒ­é—¨ç‰©å“ (Warm+Hot) å æ¯”: {hot_ratio*100:.2f}%")
    
    if hot_ratio > 0.5:
        print("   ğŸ”´ ç»“è®º: æµ‹è¯•é›†è¢«ã€çƒ­é—¨ç‰©å“ã€‘ä¸»å¯¼äº†ï¼")
        print("   è¿™å°±åƒæ˜¯åœ¨è€ƒâ€˜é•¿å°¾åˆ†å¸ƒâ€™ï¼Œä½†å·å­é‡Œ 80% çš„é¢˜éƒ½æ˜¯é€åˆ†é¢˜ã€‚")
        print("   SASRec æ‹¿é«˜åˆ†æ˜¯å› ä¸ºå®ƒåªåšäº†é€åˆ†é¢˜ã€‚")
        print("   K-GNN çš„ä»·å€¼åœ¨äºé‚£ 20% çš„éš¾é¢˜ï¼Œä½†è¢«å¹³å‡åˆ†æ·¹æ²¡äº†ã€‚")
    else:
        print("   ğŸŸ¢ ç»“è®º: æµ‹è¯•é›†åˆ†å¸ƒç›¸å¯¹å‡è¡¡ï¼ŒK-GNN ç†åº”åœ¨æ€»åˆ†ä¸Šæœ‰ä½“ç°ã€‚")

if __name__ == "__main__":
    analyze_performance_by_popularity()